{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Įvairių formatų duomenų failų įkėlimas, konvertavimas ir išsaugojimas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAS `.sas7bdat` formato failų skaitymas panaudojant _Python_ paketą `sas7bdat`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importuojame _Python_ paketą `sas7bdat`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sas7bdat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_SAS_ `.sas7bdat` formato failą nuskaitome ir paverčiame į _Python_ [`pandas`](http://pandas.pydata.org/) [`DataFrame`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html) objektą:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sasFilename = 'data/CAB1.sas7bdat'\n",
    "with sas7bdat.SAS7BDAT(sasFilename) as f:\n",
    "    pandasDf = f.to_data_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.804823</td>\n",
       "      <td>-0.079915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.396577</td>\n",
       "      <td>-1.083318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2.238294</td>\n",
       "      <td>-0.624232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.513658</td>\n",
       "      <td>-0.086609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.594179</td>\n",
       "      <td>0.031891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.737799</td>\n",
       "      <td>-0.250139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0.685005</td>\n",
       "      <td>-0.804158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.744281</td>\n",
       "      <td>-0.795503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0.340711</td>\n",
       "      <td>-0.300510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.349847</td>\n",
       "      <td>0.432705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>1.305716</td>\n",
       "      <td>1.425127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.415801</td>\n",
       "      <td>1.614381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.057726</td>\n",
       "      <td>-0.948333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>0.953648</td>\n",
       "      <td>0.391979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.076141</td>\n",
       "      <td>1.220557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.630841</td>\n",
       "      <td>-0.635758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.340003</td>\n",
       "      <td>-0.076284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>0.965358</td>\n",
       "      <td>-1.216698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>1.184490</td>\n",
       "      <td>-0.343694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>1.090243</td>\n",
       "      <td>-0.135313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.359502</td>\n",
       "      <td>-2.331338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.409686</td>\n",
       "      <td>0.654201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>0.399257</td>\n",
       "      <td>-0.469306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>0.866329</td>\n",
       "      <td>-0.923723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>1.277463</td>\n",
       "      <td>-1.452603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>0.348922</td>\n",
       "      <td>-0.055347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.227999</td>\n",
       "      <td>0.145022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.839501</td>\n",
       "      <td>-0.096262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.857583</td>\n",
       "      <td>0.764975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>0.045009</td>\n",
       "      <td>1.928383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>3</td>\n",
       "      <td>3.068341</td>\n",
       "      <td>9.705380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>3</td>\n",
       "      <td>5.828051</td>\n",
       "      <td>9.632602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>3</td>\n",
       "      <td>2.936345</td>\n",
       "      <td>8.234908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>3</td>\n",
       "      <td>4.557131</td>\n",
       "      <td>8.550686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>3</td>\n",
       "      <td>3.503758</td>\n",
       "      <td>7.755109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>3</td>\n",
       "      <td>4.107537</td>\n",
       "      <td>8.427274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>3</td>\n",
       "      <td>4.475027</td>\n",
       "      <td>8.039227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>3</td>\n",
       "      <td>3.926934</td>\n",
       "      <td>8.079344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>3</td>\n",
       "      <td>3.472544</td>\n",
       "      <td>7.935878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>3</td>\n",
       "      <td>5.925297</td>\n",
       "      <td>9.003020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>3</td>\n",
       "      <td>3.245044</td>\n",
       "      <td>9.264734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>3</td>\n",
       "      <td>2.922313</td>\n",
       "      <td>8.040794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>3</td>\n",
       "      <td>4.474726</td>\n",
       "      <td>8.592912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>3</td>\n",
       "      <td>3.888893</td>\n",
       "      <td>7.751160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>3</td>\n",
       "      <td>3.499431</td>\n",
       "      <td>7.028060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>3</td>\n",
       "      <td>4.902451</td>\n",
       "      <td>8.639766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>3</td>\n",
       "      <td>4.696278</td>\n",
       "      <td>8.600141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>3</td>\n",
       "      <td>2.973148</td>\n",
       "      <td>7.664024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>3</td>\n",
       "      <td>4.386240</td>\n",
       "      <td>8.178177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>3</td>\n",
       "      <td>4.084195</td>\n",
       "      <td>8.128002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>3</td>\n",
       "      <td>3.326874</td>\n",
       "      <td>7.032970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>3</td>\n",
       "      <td>3.662543</td>\n",
       "      <td>7.724472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>3</td>\n",
       "      <td>4.012212</td>\n",
       "      <td>8.365216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>3</td>\n",
       "      <td>5.430587</td>\n",
       "      <td>6.472152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>3</td>\n",
       "      <td>3.432358</td>\n",
       "      <td>11.014929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>3</td>\n",
       "      <td>2.679753</td>\n",
       "      <td>7.761338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>3</td>\n",
       "      <td>4.397354</td>\n",
       "      <td>7.636179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>3</td>\n",
       "      <td>3.543752</td>\n",
       "      <td>8.034934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>3</td>\n",
       "      <td>3.623486</td>\n",
       "      <td>8.183932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>3</td>\n",
       "      <td>3.041563</td>\n",
       "      <td>7.313080</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     c         x          y\n",
       "0    1  1.804823  -0.079915\n",
       "1    1  0.396577  -1.083318\n",
       "2    1  2.238294  -0.624232\n",
       "3    1  0.513658  -0.086609\n",
       "4    1 -0.594179   0.031891\n",
       "5    1 -0.737799  -0.250139\n",
       "6    1  0.685005  -0.804158\n",
       "7    1 -0.744281  -0.795503\n",
       "8    1  0.340711  -0.300510\n",
       "9    1 -1.349847   0.432705\n",
       "10   1  1.305716   1.425127\n",
       "11   1 -0.415801   1.614381\n",
       "12   1 -1.057726  -0.948333\n",
       "13   1  0.953648   0.391979\n",
       "14   1 -0.076141   1.220557\n",
       "15   1 -0.630841  -0.635758\n",
       "16   1 -0.340003  -0.076284\n",
       "17   1  0.965358  -1.216698\n",
       "18   1  1.184490  -0.343694\n",
       "19   1  1.090243  -0.135313\n",
       "20   1 -1.359502  -2.331338\n",
       "21   1 -0.409686   0.654201\n",
       "22   1  0.399257  -0.469306\n",
       "23   1  0.866329  -0.923723\n",
       "24   1  1.277463  -1.452603\n",
       "25   1  0.348922  -0.055347\n",
       "26   1 -1.227999   0.145022\n",
       "27   1 -0.839501  -0.096262\n",
       "28   1 -0.857583   0.764975\n",
       "29   1  0.045009   1.928383\n",
       "..  ..       ...        ...\n",
       "150  3  3.068341   9.705380\n",
       "151  3  5.828051   9.632602\n",
       "152  3  2.936345   8.234908\n",
       "153  3  4.557131   8.550686\n",
       "154  3  3.503758   7.755109\n",
       "155  3  4.107537   8.427274\n",
       "156  3  4.475027   8.039227\n",
       "157  3  3.926934   8.079344\n",
       "158  3  3.472544   7.935878\n",
       "159  3  5.925297   9.003020\n",
       "160  3  3.245044   9.264734\n",
       "161  3  2.922313   8.040794\n",
       "162  3  4.474726   8.592912\n",
       "163  3  3.888893   7.751160\n",
       "164  3  3.499431   7.028060\n",
       "165  3  4.902451   8.639766\n",
       "166  3  4.696278   8.600141\n",
       "167  3  2.973148   7.664024\n",
       "168  3  4.386240   8.178177\n",
       "169  3  4.084195   8.128002\n",
       "170  3  3.326874   7.032970\n",
       "171  3  3.662543   7.724472\n",
       "172  3  4.012212   8.365216\n",
       "173  3  5.430587   6.472152\n",
       "174  3  3.432358  11.014929\n",
       "175  3  2.679753   7.761338\n",
       "176  3  4.397354   7.636179\n",
       "177  3  3.543752   8.034934\n",
       "178  3  3.623486   8.183932\n",
       "179  3  3.041563   7.313080\n",
       "\n",
       "[180 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandasDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pandas.DataFrame` objekto turinį įrašome į `.csv` formato failą."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/CAB1.csv'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csvFilename = sasFilename.replace('.sas7bdat', '.csv')\n",
    "csvFilename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pandasDf.to_csv(csvFilename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Išvedame `.csv` failo turinį panaudodami _linux_ komandą `cat`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",c,x,y\r\n",
      "0,1.0,1.8048229506419473,-0.07991502090275329\r\n",
      "1,1.0,0.39657685504793405,-1.0833176549619468\r\n",
      "2,1.0,2.2382943650985343,-0.6242322943044329\r\n",
      "3,1.0,0.5136577082925808,-0.08660911687902768\r\n",
      "4,1.0,-0.5941787333790818,0.031890818063799654\r\n",
      "5,1.0,-0.7377985721302719,-0.25013917494627913\r\n",
      "6,1.0,0.6850047647381264,-0.8041581316224685\r\n",
      "7,1.0,-0.7442810802984545,-0.7955028221064093\r\n",
      "8,1.0,0.3407105493184896,-0.3005098035802357\r\n",
      "9,1.0,-1.3498465156274604,0.4327048610453688\r\n",
      "10,1.0,1.305716242646934,1.4251270103591331\r\n",
      "11,1.0,-0.415801019499061,1.6143805422533701\r\n",
      "12,1.0,-1.057726424098238,-0.9483326721764074\r\n",
      "13,1.0,0.9536476400272708,0.39197894163906866\r\n",
      "14,1.0,-0.07614127913511942,1.2205569287348228\r\n",
      "15,1.0,-0.6308414189073877,-0.6357582472716945\r\n",
      "16,1.0,-0.34000283342507026,-0.07628363677239759\r\n",
      "17,1.0,0.9653583786044433,-1.2166983927386992\r\n",
      "18,1.0,1.184490002218949,-0.3436939492474335\r\n",
      "19,1.0,1.090242787223093,-0.13531306784080646\r\n",
      "20,1.0,-1.359501932856284,-2.331337587082326\r\n",
      "21,1.0,-0.40968631040949993,0.6542014011456639\r\n",
      "22,1.0,0.399257098175889,-0.46930633886245665\r\n",
      "23,1.0,0.8663289182445575,-0.9237234966837641\r\n",
      "24,1.0,1.2774625048345505,-1.452602630134054\r\n",
      "25,1.0,0.3489217786009371,-0.05534732126176289\r\n",
      "26,1.0,-1.2279987979613458,0.14502200902608198\r\n",
      "27,1.0,-0.8395007628674837,-0.09626220831893306\r\n",
      "28,1.0,-0.8575827874050501,0.7649745830017121\r\n",
      "29,1.0,0.045008731996498935,1.928383220102947\r\n",
      "30,1.0,-0.34643160509333715,1.6059491195964959\r\n",
      "31,1.0,-1.0214087568703143,-0.2746527228387703\r\n",
      "32,1.0,0.17950280265028987,-0.8211622783507506\r\n",
      "33,1.0,0.4741141924469747,-1.5352129224828128\r\n",
      "34,1.0,0.7799456266791599,0.8612692222111332\r\n",
      "35,1.0,0.3474949210667344,0.33979491738165457\r\n",
      "36,1.0,-1.0619626178403248,-0.5374052034037823\r\n",
      "37,1.0,0.5737402760948112,0.7194025165952412\r\n",
      "38,1.0,1.3796575144393386,-0.9944391295559194\r\n",
      "39,1.0,-0.8397882159641256,0.7978011520778004\r\n",
      "40,1.0,1.3786037625681535,-0.9946695835796702\r\n",
      "41,1.0,-0.6276085106849755,-0.12234638267002784\r\n",
      "42,1.0,0.7362932503184302,0.8966411574840135\r\n",
      "43,1.0,-0.40873936648647063,-0.11147027134028516\r\n",
      "44,1.0,1.041244783040574,-0.9989783609679661\r\n",
      "45,1.0,-0.052306206920787336,1.4995985111941128\r\n",
      "46,1.0,-0.6197014623285967,-1.3122005209544743\r\n",
      "47,1.0,-0.4520491892691858,-0.9529840630973088\r\n",
      "48,1.0,-2.484927444638573,-0.8265549456600964\r\n",
      "49,1.0,-0.17600302891369374,1.2984788266860081\r\n",
      "50,1.0,0.2525898477543982,0.6990599074256625\r\n",
      "51,1.0,-0.05148527069092918,0.1952199244268394\r\n",
      "52,1.0,-1.5117785519070808,-0.3650434258601983\r\n",
      "53,1.0,0.32320622748077665,-0.9692146576202678\r\n",
      "54,1.0,-1.7174748089495508,0.11432421049113059\r\n",
      "55,1.0,0.994262995329032,0.8300953287489883\r\n",
      "56,1.0,0.8337445492022227,-0.28257136224672513\r\n",
      "57,1.0,1.2375162953149375,1.1144650260940485\r\n",
      "58,1.0,-0.9151566621094741,-0.3350095148025325\r\n",
      "59,1.0,-1.6252513079311424,-0.8050922888979747\r\n",
      "60,2.0,8.77117938490041,0.26963090862588485\r\n",
      "61,2.0,8.477535440666166,-0.7156321089004164\r\n",
      "62,2.0,6.8390833268746265,-0.23556165237292542\r\n",
      "63,2.0,7.843597466352737,0.027977004854244973\r\n",
      "64,2.0,6.943590716492299,0.7282929716254922\r\n",
      "65,2.0,8.327110276144207,0.3527267181967888\r\n",
      "66,2.0,8.123519939159976,-0.5886298246616184\r\n",
      "67,2.0,8.353011863330591,1.1106755816289382\r\n",
      "68,2.0,9.157190420360653,-0.9394945806562497\r\n",
      "69,2.0,7.07950582441678,1.3752189606314391\r\n",
      "70,2.0,7.230885102041306,0.3052369050836137\r\n",
      "71,2.0,8.251341612844527,-0.08036873485221664\r\n",
      "72,2.0,7.748528835528485,-2.236053406988492\r\n",
      "73,2.0,9.740151270703311,0.7703542774031303\r\n",
      "74,2.0,8.08553877976148,1.6595860529710798\r\n",
      "75,2.0,7.4452453102188,0.3412945583794203\r\n",
      "76,2.0,9.556202187592405,-0.208671190382685\r\n",
      "77,2.0,8.009009601800798,1.6194167745187493\r\n",
      "78,2.0,7.267002498801442,-0.311961333676985\r\n",
      "79,2.0,8.228538118831626,-0.41309146576802236\r\n",
      "80,2.0,7.747647600412615,-0.09882043037739613\r\n",
      "81,2.0,8.439806312258396,0.14633004959800044\r\n",
      "82,2.0,8.483379000150654,-0.5152222059371192\r\n",
      "83,2.0,8.408774460408603,-1.6763550909140217\r\n",
      "84,2.0,7.782746686996818,0.5914129020026195\r\n",
      "85,2.0,8.749309577583894,-0.3090855723082963\r\n",
      "86,2.0,8.135317520464104,1.4197987009639859\r\n",
      "87,2.0,9.313154335469754,-1.682273213395627\r\n",
      "88,2.0,7.701325490002836,0.12008873488167271\r\n",
      "89,2.0,7.680727420918149,-0.6589327429453704\r\n",
      "90,2.0,8.20883852857128,-0.9786832953857302\r\n",
      "91,2.0,7.8677972306189075,3.2444009124896214\r\n",
      "92,2.0,6.813089392430121,0.12989474450547422\r\n",
      "93,2.0,9.255204468939647,-0.22289457407196658\r\n",
      "94,2.0,8.10970436067399,-1.2182890506030806\r\n",
      "95,2.0,8.881967669637948,0.22760760634467492\r\n",
      "96,2.0,7.774452321446848,2.102180397296799\r\n",
      "97,2.0,8.098648798632391,-0.6046667864442006\r\n",
      "98,2.0,9.038734205646541,1.203078906280366\r\n",
      "99,2.0,9.754990223877915,1.6785938745346403\r\n",
      "100,2.0,6.089771993506552,-2.783195225400488\r\n",
      "101,2.0,8.38155087230137,-2.2103815529314246\r\n",
      "102,2.0,8.90926116519618,2.0450756891590647\r\n",
      "103,2.0,8.201527871606128,0.397931399206435\r\n",
      "104,2.0,7.277856364140053,-0.28398816631252544\r\n",
      "105,2.0,6.244910552953375,0.8735928291747974\r\n",
      "106,2.0,8.307195186094068,-0.05895920005981419\r\n",
      "107,2.0,6.953882797995391,-1.154532022005487\r\n",
      "108,2.0,7.911889166650834,1.0962709653536502\r\n",
      "109,2.0,7.357350968669488,1.9888271089832605\r\n",
      "110,2.0,7.091854310265283,0.47419788499939275\r\n",
      "111,2.0,7.968585574037946,-0.24890589566933832\r\n",
      "112,2.0,7.4078007268347115,0.21808954247838053\r\n",
      "113,2.0,7.8109973732126114,0.33247365429043535\r\n",
      "114,2.0,7.698760745319813,1.5626735226883115\r\n",
      "115,2.0,8.19290431989854,-0.9602920092197902\r\n",
      "116,2.0,7.309142979249098,-0.24046668371720747\r\n",
      "117,2.0,6.689419754148963,-1.7119038763949044\r\n",
      "118,2.0,7.253082469851929,1.710538617574112\r\n",
      "119,2.0,7.130153309710952,1.1905226449789497\r\n",
      "120,3.0,1.7003309448339596,6.798768110803229\r\n",
      "121,3.0,2.484776504889419,7.313637906287766\r\n",
      "122,3.0,3.6444871270771353,8.959709616960982\r\n",
      "123,3.0,5.095348464845727,7.949862060874071\r\n",
      "124,3.0,3.8258302076845383,7.181975491917652\r\n",
      "125,3.0,1.8264265334330876,6.739364004585627\r\n",
      "126,3.0,4.684136625595761,8.306402459247375\r\n",
      "127,3.0,3.39972217268821,6.521770699821907\r\n",
      "128,3.0,3.2331710915274847,8.447941901272971\r\n",
      "129,3.0,4.264253793908269,5.437254941532196\r\n",
      "130,3.0,4.793626140701928,7.602371858205171\r\n",
      "131,3.0,3.939459416323571,7.050602944601121\r\n",
      "132,3.0,4.662503707217861,8.089133084952373\r\n",
      "133,3.0,4.454128747905958,8.256771107718583\r\n",
      "134,3.0,2.05555211749535,6.874106693309752\r\n",
      "135,3.0,5.187833722758501,7.909655796200556\r\n",
      "136,3.0,2.629502582881419,9.416150655500367\r\n",
      "137,3.0,4.744369235347512,7.175887890265736\r\n",
      "138,3.0,4.29198668073107,7.695210212584311\r\n",
      "139,3.0,3.3283670859936003,7.451840752861451\r\n",
      "140,3.0,3.9202076103949293,8.607076078592996\r\n",
      "141,3.0,5.086715956158335,8.32214170068701\r\n",
      "142,3.0,3.157640037222086,8.91837658194547\r\n",
      "143,3.0,4.857423250042348,7.116841038691952\r\n",
      "144,3.0,4.614149286714087,6.645974554492941\r\n",
      "145,3.0,3.5964725993548012,8.838763646786454\r\n",
      "146,3.0,3.143978833658354,8.287160492162746\r\n",
      "147,3.0,3.0292386651419365,7.139005261972294\r\n",
      "148,3.0,3.9820466836880324,9.553344690121367\r\n",
      "149,3.0,3.891678498919164,10.309754132336678\r\n",
      "150,3.0,3.068341097062113,9.705379807076797\r\n",
      "151,3.0,5.8280506886774095,9.632601646851274\r\n",
      "152,3.0,2.936345101271549,8.23490783525004\r\n",
      "153,3.0,4.557130875252462,8.550686244834552\r\n",
      "154,3.0,3.503757850830543,7.755109180159972\r\n",
      "155,3.0,4.107536799560873,8.427274326377011\r\n",
      "156,3.0,4.475026770339942,8.03922744418119\r\n",
      "157,3.0,3.926934173234619,8.079343725319308\r\n",
      "158,3.0,3.472543852929225,7.935878253986387\r\n",
      "159,3.0,5.925296779662469,9.00302003906\r\n",
      "160,3.0,3.245043985792252,9.264733858968047\r\n",
      "161,3.0,2.922313318523054,8.040794316501318\r\n",
      "162,3.0,4.474726455482037,8.592912432562164\r\n",
      "163,3.0,3.888893329226865,7.7511602521118235\r\n",
      "164,3.0,3.499430968997595,7.0280599649485005\r\n",
      "165,3.0,4.902450798926265,8.639766434601512\r\n",
      "166,3.0,4.696278139024929,8.60014114096224\r\n",
      "167,3.0,2.973147922316791,7.664023536075025\r\n",
      "168,3.0,4.386239760642813,8.178177114409623\r\n",
      "169,3.0,4.08419514834024,8.128002336801865\r\n",
      "170,3.0,3.326874447325465,7.032969597237878\r\n",
      "171,3.0,3.662543424297694,7.724472311617789\r\n",
      "172,3.0,4.012212124569602,8.365215625717276\r\n",
      "173,3.0,5.430586707977409,6.472151835543553\r\n",
      "174,3.0,3.432358007764408,11.014928767795492\r\n",
      "175,3.0,2.67975300845006,7.761338115509186\r\n",
      "176,3.0,4.397353690934557,7.636178854756983\r\n",
      "177,3.0,3.54375161727691,8.034933973331546\r\n",
      "178,3.0,3.6234857972947982,8.183932181621879\r\n",
      "179,3.0,3.0415630717919964,7.313080281060777\r\n"
     ]
    }
   ],
   "source": [
    "! cat data/CAB1.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pastebime, kad pirmasis išsaugotas stulpelis neturi pavadinimo. Jo reikšmės yra `pandas.DataFrame` indeksas. Šis indeksas buvo sukurtas duomenų įkėlimo metu ir saugojant duomenis mums nėra reikalingas. \n",
    "\n",
    "Parametru `index=None` nurodome, kad nenorime saugoti indekso stulpelio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pandasDf.to_csv(csvFilename, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dar kartą išvedame `.csv` failo turinį panaudodami _linux_ komandą `cat`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c,x,y\r\n",
      "1.0,1.8048229506419473,-0.07991502090275329\r\n",
      "1.0,0.39657685504793405,-1.0833176549619468\r\n",
      "1.0,2.2382943650985343,-0.6242322943044329\r\n",
      "1.0,0.5136577082925808,-0.08660911687902768\r\n",
      "1.0,-0.5941787333790818,0.031890818063799654\r\n",
      "1.0,-0.7377985721302719,-0.25013917494627913\r\n",
      "1.0,0.6850047647381264,-0.8041581316224685\r\n",
      "1.0,-0.7442810802984545,-0.7955028221064093\r\n",
      "1.0,0.3407105493184896,-0.3005098035802357\r\n",
      "1.0,-1.3498465156274604,0.4327048610453688\r\n",
      "1.0,1.305716242646934,1.4251270103591331\r\n",
      "1.0,-0.415801019499061,1.6143805422533701\r\n",
      "1.0,-1.057726424098238,-0.9483326721764074\r\n",
      "1.0,0.9536476400272708,0.39197894163906866\r\n",
      "1.0,-0.07614127913511942,1.2205569287348228\r\n",
      "1.0,-0.6308414189073877,-0.6357582472716945\r\n",
      "1.0,-0.34000283342507026,-0.07628363677239759\r\n",
      "1.0,0.9653583786044433,-1.2166983927386992\r\n",
      "1.0,1.184490002218949,-0.3436939492474335\r\n",
      "1.0,1.090242787223093,-0.13531306784080646\r\n",
      "1.0,-1.359501932856284,-2.331337587082326\r\n",
      "1.0,-0.40968631040949993,0.6542014011456639\r\n",
      "1.0,0.399257098175889,-0.46930633886245665\r\n",
      "1.0,0.8663289182445575,-0.9237234966837641\r\n",
      "1.0,1.2774625048345505,-1.452602630134054\r\n",
      "1.0,0.3489217786009371,-0.05534732126176289\r\n",
      "1.0,-1.2279987979613458,0.14502200902608198\r\n",
      "1.0,-0.8395007628674837,-0.09626220831893306\r\n",
      "1.0,-0.8575827874050501,0.7649745830017121\r\n",
      "1.0,0.045008731996498935,1.928383220102947\r\n",
      "1.0,-0.34643160509333715,1.6059491195964959\r\n",
      "1.0,-1.0214087568703143,-0.2746527228387703\r\n",
      "1.0,0.17950280265028987,-0.8211622783507506\r\n",
      "1.0,0.4741141924469747,-1.5352129224828128\r\n",
      "1.0,0.7799456266791599,0.8612692222111332\r\n",
      "1.0,0.3474949210667344,0.33979491738165457\r\n",
      "1.0,-1.0619626178403248,-0.5374052034037823\r\n",
      "1.0,0.5737402760948112,0.7194025165952412\r\n",
      "1.0,1.3796575144393386,-0.9944391295559194\r\n",
      "1.0,-0.8397882159641256,0.7978011520778004\r\n",
      "1.0,1.3786037625681535,-0.9946695835796702\r\n",
      "1.0,-0.6276085106849755,-0.12234638267002784\r\n",
      "1.0,0.7362932503184302,0.8966411574840135\r\n",
      "1.0,-0.40873936648647063,-0.11147027134028516\r\n",
      "1.0,1.041244783040574,-0.9989783609679661\r\n",
      "1.0,-0.052306206920787336,1.4995985111941128\r\n",
      "1.0,-0.6197014623285967,-1.3122005209544743\r\n",
      "1.0,-0.4520491892691858,-0.9529840630973088\r\n",
      "1.0,-2.484927444638573,-0.8265549456600964\r\n",
      "1.0,-0.17600302891369374,1.2984788266860081\r\n",
      "1.0,0.2525898477543982,0.6990599074256625\r\n",
      "1.0,-0.05148527069092918,0.1952199244268394\r\n",
      "1.0,-1.5117785519070808,-0.3650434258601983\r\n",
      "1.0,0.32320622748077665,-0.9692146576202678\r\n",
      "1.0,-1.7174748089495508,0.11432421049113059\r\n",
      "1.0,0.994262995329032,0.8300953287489883\r\n",
      "1.0,0.8337445492022227,-0.28257136224672513\r\n",
      "1.0,1.2375162953149375,1.1144650260940485\r\n",
      "1.0,-0.9151566621094741,-0.3350095148025325\r\n",
      "1.0,-1.6252513079311424,-0.8050922888979747\r\n",
      "2.0,8.77117938490041,0.26963090862588485\r\n",
      "2.0,8.477535440666166,-0.7156321089004164\r\n",
      "2.0,6.8390833268746265,-0.23556165237292542\r\n",
      "2.0,7.843597466352737,0.027977004854244973\r\n",
      "2.0,6.943590716492299,0.7282929716254922\r\n",
      "2.0,8.327110276144207,0.3527267181967888\r\n",
      "2.0,8.123519939159976,-0.5886298246616184\r\n",
      "2.0,8.353011863330591,1.1106755816289382\r\n",
      "2.0,9.157190420360653,-0.9394945806562497\r\n",
      "2.0,7.07950582441678,1.3752189606314391\r\n",
      "2.0,7.230885102041306,0.3052369050836137\r\n",
      "2.0,8.251341612844527,-0.08036873485221664\r\n",
      "2.0,7.748528835528485,-2.236053406988492\r\n",
      "2.0,9.740151270703311,0.7703542774031303\r\n",
      "2.0,8.08553877976148,1.6595860529710798\r\n",
      "2.0,7.4452453102188,0.3412945583794203\r\n",
      "2.0,9.556202187592405,-0.208671190382685\r\n",
      "2.0,8.009009601800798,1.6194167745187493\r\n",
      "2.0,7.267002498801442,-0.311961333676985\r\n",
      "2.0,8.228538118831626,-0.41309146576802236\r\n",
      "2.0,7.747647600412615,-0.09882043037739613\r\n",
      "2.0,8.439806312258396,0.14633004959800044\r\n",
      "2.0,8.483379000150654,-0.5152222059371192\r\n",
      "2.0,8.408774460408603,-1.6763550909140217\r\n",
      "2.0,7.782746686996818,0.5914129020026195\r\n",
      "2.0,8.749309577583894,-0.3090855723082963\r\n",
      "2.0,8.135317520464104,1.4197987009639859\r\n",
      "2.0,9.313154335469754,-1.682273213395627\r\n",
      "2.0,7.701325490002836,0.12008873488167271\r\n",
      "2.0,7.680727420918149,-0.6589327429453704\r\n",
      "2.0,8.20883852857128,-0.9786832953857302\r\n",
      "2.0,7.8677972306189075,3.2444009124896214\r\n",
      "2.0,6.813089392430121,0.12989474450547422\r\n",
      "2.0,9.255204468939647,-0.22289457407196658\r\n",
      "2.0,8.10970436067399,-1.2182890506030806\r\n",
      "2.0,8.881967669637948,0.22760760634467492\r\n",
      "2.0,7.774452321446848,2.102180397296799\r\n",
      "2.0,8.098648798632391,-0.6046667864442006\r\n",
      "2.0,9.038734205646541,1.203078906280366\r\n",
      "2.0,9.754990223877915,1.6785938745346403\r\n",
      "2.0,6.089771993506552,-2.783195225400488\r\n",
      "2.0,8.38155087230137,-2.2103815529314246\r\n",
      "2.0,8.90926116519618,2.0450756891590647\r\n",
      "2.0,8.201527871606128,0.397931399206435\r\n",
      "2.0,7.277856364140053,-0.28398816631252544\r\n",
      "2.0,6.244910552953375,0.8735928291747974\r\n",
      "2.0,8.307195186094068,-0.05895920005981419\r\n",
      "2.0,6.953882797995391,-1.154532022005487\r\n",
      "2.0,7.911889166650834,1.0962709653536502\r\n",
      "2.0,7.357350968669488,1.9888271089832605\r\n",
      "2.0,7.091854310265283,0.47419788499939275\r\n",
      "2.0,7.968585574037946,-0.24890589566933832\r\n",
      "2.0,7.4078007268347115,0.21808954247838053\r\n",
      "2.0,7.8109973732126114,0.33247365429043535\r\n",
      "2.0,7.698760745319813,1.5626735226883115\r\n",
      "2.0,8.19290431989854,-0.9602920092197902\r\n",
      "2.0,7.309142979249098,-0.24046668371720747\r\n",
      "2.0,6.689419754148963,-1.7119038763949044\r\n",
      "2.0,7.253082469851929,1.710538617574112\r\n",
      "2.0,7.130153309710952,1.1905226449789497\r\n",
      "3.0,1.7003309448339596,6.798768110803229\r\n",
      "3.0,2.484776504889419,7.313637906287766\r\n",
      "3.0,3.6444871270771353,8.959709616960982\r\n",
      "3.0,5.095348464845727,7.949862060874071\r\n",
      "3.0,3.8258302076845383,7.181975491917652\r\n",
      "3.0,1.8264265334330876,6.739364004585627\r\n",
      "3.0,4.684136625595761,8.306402459247375\r\n",
      "3.0,3.39972217268821,6.521770699821907\r\n",
      "3.0,3.2331710915274847,8.447941901272971\r\n",
      "3.0,4.264253793908269,5.437254941532196\r\n",
      "3.0,4.793626140701928,7.602371858205171\r\n",
      "3.0,3.939459416323571,7.050602944601121\r\n",
      "3.0,4.662503707217861,8.089133084952373\r\n",
      "3.0,4.454128747905958,8.256771107718583\r\n",
      "3.0,2.05555211749535,6.874106693309752\r\n",
      "3.0,5.187833722758501,7.909655796200556\r\n",
      "3.0,2.629502582881419,9.416150655500367\r\n",
      "3.0,4.744369235347512,7.175887890265736\r\n",
      "3.0,4.29198668073107,7.695210212584311\r\n",
      "3.0,3.3283670859936003,7.451840752861451\r\n",
      "3.0,3.9202076103949293,8.607076078592996\r\n",
      "3.0,5.086715956158335,8.32214170068701\r\n",
      "3.0,3.157640037222086,8.91837658194547\r\n",
      "3.0,4.857423250042348,7.116841038691952\r\n",
      "3.0,4.614149286714087,6.645974554492941\r\n",
      "3.0,3.5964725993548012,8.838763646786454\r\n",
      "3.0,3.143978833658354,8.287160492162746\r\n",
      "3.0,3.0292386651419365,7.139005261972294\r\n",
      "3.0,3.9820466836880324,9.553344690121367\r\n",
      "3.0,3.891678498919164,10.309754132336678\r\n",
      "3.0,3.068341097062113,9.705379807076797\r\n",
      "3.0,5.8280506886774095,9.632601646851274\r\n",
      "3.0,2.936345101271549,8.23490783525004\r\n",
      "3.0,4.557130875252462,8.550686244834552\r\n",
      "3.0,3.503757850830543,7.755109180159972\r\n",
      "3.0,4.107536799560873,8.427274326377011\r\n",
      "3.0,4.475026770339942,8.03922744418119\r\n",
      "3.0,3.926934173234619,8.079343725319308\r\n",
      "3.0,3.472543852929225,7.935878253986387\r\n",
      "3.0,5.925296779662469,9.00302003906\r\n",
      "3.0,3.245043985792252,9.264733858968047\r\n",
      "3.0,2.922313318523054,8.040794316501318\r\n",
      "3.0,4.474726455482037,8.592912432562164\r\n",
      "3.0,3.888893329226865,7.7511602521118235\r\n",
      "3.0,3.499430968997595,7.0280599649485005\r\n",
      "3.0,4.902450798926265,8.639766434601512\r\n",
      "3.0,4.696278139024929,8.60014114096224\r\n",
      "3.0,2.973147922316791,7.664023536075025\r\n",
      "3.0,4.386239760642813,8.178177114409623\r\n",
      "3.0,4.08419514834024,8.128002336801865\r\n",
      "3.0,3.326874447325465,7.032969597237878\r\n",
      "3.0,3.662543424297694,7.724472311617789\r\n",
      "3.0,4.012212124569602,8.365215625717276\r\n",
      "3.0,5.430586707977409,6.472151835543553\r\n",
      "3.0,3.432358007764408,11.014928767795492\r\n",
      "3.0,2.67975300845006,7.761338115509186\r\n",
      "3.0,4.397353690934557,7.636178854756983\r\n",
      "3.0,3.54375161727691,8.034933973331546\r\n",
      "3.0,3.6234857972947982,8.183932181621879\r\n",
      "3.0,3.0415630717919964,7.313080281060777\r\n"
     ]
    }
   ],
   "source": [
    "! cat data/CAB1.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dabar išsaugojome tik duomenis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Pastaba_**: _Jupyter Notebook_ _Code_ rūšies ląstelėje įrašę `!` simbolį galime naudoti _linux_ komandas, pvz:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Išvesti darbinės direktorijos turinį:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\t   examples_datafiles.ipynb\t .gitignore\t     README.md\r\n",
      "..\t   examples_kmeans.ipynb\t hs_err_pid5118.log\r\n",
      "data\t   examples_kmeans_tmp_ml.ipynb  .ipynb_checkpoints\r\n",
      "derby.log  .git\t\t\t\t metastore_db\r\n"
     ]
    }
   ],
   "source": [
    "! ls -a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Pastaba:_** _Jupyter Notebook_ darbinė direktorija yra direktorija, kurioje patalpintas dabartinis bloknoto `.ipynb` failas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vagrant/labs/p160m132-examples-spark\r\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sukurti naują direktoriją `nauja_direktorija` ir išvesti darbinės direktorijos turinį:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\t\t\t  examples_kmeans.ipynb\t\tmetastore_db\r\n",
      "derby.log\t\t  examples_kmeans_tmp_ml.ipynb\tnauja_direktorija\r\n",
      "examples_datafiles.ipynb  hs_err_pid5118.log\t\tREADME.md\r\n"
     ]
    }
   ],
   "source": [
    "! mkdir nauja_direktorija\n",
    "! ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ištrinti direktoriją `nauja_direktorija` ir išvesti darbinės direktorijos turinį:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\t   examples_datafiles.ipynb  examples_kmeans_tmp_ml.ipynb  metastore_db\r\n",
      "derby.log  examples_kmeans.ipynb     hs_err_pid5118.log\t\t   README.md\r\n"
     ]
    }
   ],
   "source": [
    "! rm -rf nauja_direktorija\n",
    "! ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nadojant simbolį `|` Sukurti komandų seką, kai vienos komandos išvestis tampa kitos komandos įvestimi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CA10.xls\r\n",
      "CA1.xls\r\n",
      "CA2.xls\r\n",
      "CA3.xls\r\n",
      "CA4.xls\r\n",
      "CA5.xls\r\n",
      "CA6.xls\r\n",
      "CA7.xls\r\n",
      "CA8.xls\r\n",
      "CA9.xls\r\n"
     ]
    }
   ],
   "source": [
    "! ls data | grep .xls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excel _.xls_ ir _.xlsx_ formato failų skaitymas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toliau pateikiama keletas būdų kaip įkelti _Excel_ `.xls` ir `.xlsx` formatų failus ir juos išsaugoti `.csv` formatu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python `pandas` paketas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importuojame _Python_ paketą [`pandas`](http://pandas.pydata.org/) ir pakeičiame jo pavadinimą į `pd`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>klasteris</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.485695</td>\n",
       "      <td>2.075174</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.368247</td>\n",
       "      <td>2.037708</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>-1.409078</td>\n",
       "      <td>-1.371631</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>-1.434896</td>\n",
       "      <td>-1.959709</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3.706690</td>\n",
       "      <td>-0.803241</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID         X         Y  klasteris\n",
       "0   1 -0.485695  2.075174          1\n",
       "1   2 -0.368247  2.037708          2\n",
       "2   3 -1.409078 -1.371631          3\n",
       "3   4 -1.434896 -1.959709          4\n",
       "4   5  3.706690 -0.803241          5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdDf = pd.read_excel(\"data/CA1.xls\")\n",
    "pdDf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Galime išsaugoti `df` turinį į `.csv` formato failą kaip tai darėme su įkeltu `.sas7bdat` failu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pdDf.to_csv(\"data/CA1.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Išvedame pirmas 10 failo `CA1.csv` eilučių:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID,X,Y,klasteris\r\n",
      "1,-0.485694686,2.075174209,1\r\n",
      "2,-0.3682471,2.037708135,2\r\n",
      "3,-1.409078419,-1.371631094,3\r\n",
      "4,-1.434896397,-1.959709476,4\r\n",
      "5,3.706690333,-0.803240913,5\r\n",
      "6,-0.488402521,2.076363672,1\r\n",
      "7,-0.365141811,2.053486526,2\r\n",
      "8,-1.403284478,-1.387871943,3\r\n",
      "9,-1.443300554,-1.940563955,4\r\n"
     ]
    }
   ],
   "source": [
    "! head data/CA1.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python `xlrd` paketas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`xlrd` paketas leidžia dirbti su _Excel_ failais itin žemu abstrakcijos lygmeniu. Jeigu _Excel_ failai yra tvarkingos lentelės, prasidedančios viršutiniame kairiame _Excel_ darbo knygos kampe, jų įkėlimui patogiau naudoti kitus metodus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importuojame reikalingus _Python_ paketus ir sukuriame funkciją, kuri nuskaito _Excel_ failą ir jo darbinius lakštus (angl. worksheet) išsaugo `.csv` failų formatu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xlrd\n",
    "import csv\n",
    "import os\n",
    "\n",
    "def write_csv_from_excel(excel_filepath):\n",
    "    dir_path = os.path.dirname(excel_filepath)\n",
    "    workbook = xlrd.open_workbook(excel_filepath)\n",
    "    for worksheet_name in workbook.sheet_names():\n",
    "        worksheet = workbook.sheet_by_name(worksheet_name)\n",
    "        if not worksheet.nrows:\n",
    "            continue\n",
    "        csv_filename = os.path.join(dir_path, worksheet_name) + '.csv'\n",
    "        with open(csv_filename, 'w') as csvfile:\n",
    "            csvwriter = csv.writer(csvfile, quoting=csv.QUOTE_MINIMAL)\n",
    "            for rownum in range(worksheet.nrows):\n",
    "                csvwriter.writerow(worksheet.row_values(rownum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_csv_from_excel(\"data/CA2.xls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CA10.xls  CA2.csv  CA4.xls  CA7.xls  CAB1.csv\r\n",
      "CA1.csv   CA2.xls  CA5.xls  CA8.xls  CAB1.sas7bdat\r\n",
      "CA1.xls   CA3.xls  CA6.xls  CA9.xls  Untitled.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "! ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CA2.csv\r\n",
      "CA2.xls\r\n"
     ]
    }
   ],
   "source": [
    "! ls data | grep CA2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id,X,y,klasteris\r",
      "\r\n",
      "1.0,-0.543107008398867,-2.24589337158166,1.0\r",
      "\r\n",
      "2.0,-3.05141284857379,-2.79020246961634,1.0\r",
      "\r\n",
      "3.0,-2.63406157935159,-0.90642746845988,1.0\r",
      "\r\n",
      "4.0,-2.94372086031095,-3.15672525023875,1.0\r",
      "\r\n",
      "5.0,-1.59848537397944,-4.507897764527,1.0\r",
      "\r\n",
      "6.0,-2.40908579499534,-4.51768121307842,1.0\r",
      "\r\n",
      "7.0,-1.71876500225743,-5.25533798241366,1.0\r",
      "\r\n",
      "8.0,-1.1705287055537,-2.10686355275734,1.0\r",
      "\r\n",
      "9.0,-0.558161109901716,-3.90145324975195,1.0\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! head data/CA2.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python `csvkit` paketas ir _Linux_ komandinė eilutė"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeigu sistemoje yra įdiegtas _Python_ paketas [`csvkit`](http://csvkit.readthedocs.org/en/0.9.1/), galime pasinaudoti komandine eilutės programa `in2csv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! in2csv data/CA1.xls > data/CA1_csvkit.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID,X,Y,klasteris\r\n",
      "1,-0.485694686,2.075174209,1\r\n",
      "2,-0.3682471,2.037708135,2\r\n",
      "3,-1.409078419,-1.371631094,3\r\n",
      "4,-1.434896397,-1.959709476,4\r\n",
      "5,3.706690333,-0.803240913,5\r\n",
      "6,-0.488402521,2.076363672,1\r\n",
      "7,-0.365141811,2.053486526,2\r\n",
      "8,-1.403284478,-1.387871943,3\r\n",
      "9,-1.443300554,-1.940563955,4\r\n"
     ]
    }
   ],
   "source": [
    "! head data/CA1_csvkit.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detali programos `in2csv` naudojimo dokumentacija:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: in2csv [-h] [-d DELIMITER] [-t] [-q QUOTECHAR] [-u {0,1,2,3}] [-b]\r\n",
      "              [-p ESCAPECHAR] [-z MAXFIELDSIZE] [-e ENCODING] [-S] [-H] [-v]\r\n",
      "              [-l] [--zero] [-f FILETYPE] [-s SCHEMA] [-k KEY] [-y SNIFFLIMIT]\r\n",
      "              [--sheet SHEET] [--no-inference]\r\n",
      "              [FILE]\r\n",
      "\r\n",
      "Convert common, but less awesome, tabular data formats to CSV.\r\n",
      "\r\n",
      "positional arguments:\r\n",
      "  FILE                  The CSV file to operate on. If omitted, will accept\r\n",
      "                        input on STDIN.\r\n",
      "\r\n",
      "optional arguments:\r\n",
      "  -h, --help            show this help message and exit\r\n",
      "  -d DELIMITER, --delimiter DELIMITER\r\n",
      "                        Delimiting character of the input CSV file.\r\n",
      "  -t, --tabs            Specifies that the input CSV file is delimited with\r\n",
      "                        tabs. Overrides \"-d\".\r\n",
      "  -q QUOTECHAR, --quotechar QUOTECHAR\r\n",
      "                        Character used to quote strings in the input CSV file.\r\n",
      "  -u {0,1,2,3}, --quoting {0,1,2,3}\r\n",
      "                        Quoting style used in the input CSV file. 0 = Quote\r\n",
      "                        Minimal, 1 = Quote All, 2 = Quote Non-numeric, 3 =\r\n",
      "                        Quote None.\r\n",
      "  -b, --doublequote     Whether or not double quotes are doubled in the input\r\n",
      "                        CSV file.\r\n",
      "  -p ESCAPECHAR, --escapechar ESCAPECHAR\r\n",
      "                        Character used to escape the delimiter if --quoting 3\r\n",
      "                        (\"Quote None\") is specified and to escape the\r\n",
      "                        QUOTECHAR if --doublequote is not specified.\r\n",
      "  -z MAXFIELDSIZE, --maxfieldsize MAXFIELDSIZE\r\n",
      "                        Maximum length of a single field in the input CSV\r\n",
      "                        file.\r\n",
      "  -e ENCODING, --encoding ENCODING\r\n",
      "                        Specify the encoding the input CSV file.\r\n",
      "  -S, --skipinitialspace\r\n",
      "                        Ignore whitespace immediately following the delimiter.\r\n",
      "  -H, --no-header-row   Specifies that the input CSV file has no header row.\r\n",
      "                        Will create default headers.\r\n",
      "  -v, --verbose         Print detailed tracebacks when errors occur.\r\n",
      "  -l, --linenumbers     Insert a column of line numbers at the front of the\r\n",
      "                        output. Useful when piping to grep or as a simple\r\n",
      "                        primary key.\r\n",
      "  --zero                When interpreting or displaying column numbers, use\r\n",
      "                        zero-based numbering instead of the default 1-based\r\n",
      "                        numbering.\r\n",
      "  -f FILETYPE, --format FILETYPE\r\n",
      "                        The format of the input file. If not specified will be\r\n",
      "                        inferred from the file type. Supported formats: csv,\r\n",
      "                        fixed, geojson, json, ndjson, xls, xlsx.\r\n",
      "  -s SCHEMA, --schema SCHEMA\r\n",
      "                        Specifies a CSV-formatted schema file for converting\r\n",
      "                        fixed-width files. See documentation for details.\r\n",
      "  -k KEY, --key KEY     Specifies a top-level key to use look within for a\r\n",
      "                        list of objects to be converted when processing JSON.\r\n",
      "  -y SNIFFLIMIT, --snifflimit SNIFFLIMIT\r\n",
      "                        Limit CSV dialect sniffing to the specified number of\r\n",
      "                        bytes. Specify \"0\" to disable sniffing entirely.\r\n",
      "  --sheet SHEET         The name of the XLSX sheet to operate on.\r\n",
      "  --no-inference        Disable type inference when parsing the input.\r\n",
      "\r\n",
      "Some command line flags only pertain to specific input formats.\r\n"
     ]
    }
   ],
   "source": [
    "! in2csv -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duomenys į _Apache Spark_ paprastai įkeliami iš duomenų bazių arba tekstinių failų, esančių lokalioje _Linux_ failų sistemoje arba _Hadoop_ paskirstytoje failų sistemoje (angl. Hadoop Distributed File System - [HDFS](http://hortonworks.com/hadoop/hdfs/)). Studijų modulyje **P160M132** apsiribosime darbu su tekstiniais failais. \n",
    "\n",
    "Toliau pateikiama keletas metodų kaip įkelti ir išsaugoti tekstinius failus su _Apache Spark_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tekstiniai failai lokalioje _Linux_ failų sistemoje "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `sc.textFile` metodas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ID,X,Y,klasteris',\n",
       " '1,-0.485694686,2.075174209,1',\n",
       " '2,-0.3682471,2.037708135,2',\n",
       " '3,-1.409078419,-1.371631094,3',\n",
       " '4,-1.434896397,-1.959709476,4']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csvRDD = sc.textFile(\"./data/CA1.csv\")\n",
    "print(type(csvRDD))\n",
    "csvRDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matome, kad `sc.textFile` metodas grąžina tipo `RDD` objektą. Pažiūrime kokio tipo yra `RDD` turinys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[str, str, str, str, str]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csvRDD.map(type).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nieko nuostabaus, kad įkėlę tekstinį failą, turime [`str`](https://docs.python.org/3/library/stdtypes.html#text-sequence-type-str) (tekstinių simbolių sekos) tipo objektų rinkinį. Kadangi failą skaitome naudomi _Apache Spark_, tikėtina, kad norėsime atlikti kažką daugiau, negu išvesti pirmas **N** jo eilučių. Pastarajam veiksmui puikiai tinka _Linux_ komanda `head -n` **N**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID,X,Y,klasteris\r\n",
      "1,-0.485694686,2.075174209,1\r\n",
      "2,-0.3682471,2.037708135,2\r\n",
      "3,-1.409078419,-1.371631094,3\r\n",
      "4,-1.434896397,-1.959709476,4\r\n"
     ]
    }
   ],
   "source": [
    "! head -n 5 data/CA1.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Norėdami atlikti skaičiavimus, turime atitinkamus stulpelius konvertuoti į reikiamo tipo reikšmes, tačiau mūsų pirmoji eilutė yra stupelių pavadinimai. Pirmąją eilutę paimame naudodami `RDD` metodą `first`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ID,X,Y,klasteris'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header = csvRDD.first()\n",
    "header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atskiriame pirmąją eilutę nuo `RDD`. Nors skamba paprastai, tai nėra visiškai trivialus veiksmas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1,-0.485694686,2.075174209,1',\n",
       " '2,-0.3682471,2.037708135,2',\n",
       " '3,-1.409078419,-1.371631094,3',\n",
       " '4,-1.434896397,-1.959709476,4',\n",
       " '5,3.706690333,-0.803240913,5']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools as it\n",
    "\n",
    "rowsRDD = csvRDD.mapPartitionsWithIndex(lambda idx, gen: it.islice(gen, 1, None) if idx == 0 else gen)\n",
    "\n",
    "rowsRDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prieš aptardami ką tik naudotą anoniminę funkciją, panagrinėkime kokių tipų elementai pasiekiami šiai funkcijai:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<class 'int'>\", \"<class 'generator'>\"]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csvRDD.mapPartitionsWithIndex(lambda elem1, elem2: (str(type(elem1)), str(type(elem2)))).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`mapPartitionsWithIndex` metodo argumentas yra funkcija, kuriai pateikiami 2 argumentai: `RDD` particijos indeksas ir particijos [`generator`](https://docs.python.org/3/library/stdtypes.html#generator-types) tipo objektas. _Python_ generatoriai nesaugo visų savo elementų atmintyje iš karto, tačiau juos generuoja po vieną, tik tada kai jų paprašoma.\n",
    "\n",
    "Todėl panaudojame _Python_ standartinės bibliotekos modulį `itertools`. Panaudodami [`itertools.islice`](https://docs.python.org/3/library/itertools.html?highlight=itertools%20islice#itertools.islice) klasę pirmosios particijos generatorių pakeičiame generatoriumi be pirmojo elemento.\n",
    "\n",
    "**_Pastaba_**: Atminkime, kad _Python_ indeksai prasideda nuo 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kadangi turime stulpelių pavadinimus ir duomenų eilutes atskirai, pastarąsias galime konvertuoti į skaitines reikšmes. Kad būtų aiškiau, po kiekvieno žingsnio pateiksime `RDD` elementų tipus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1', '-0.485694686', '2.075174209', '1'],\n",
       " ['2', '-0.3682471', '2.037708135', '2'],\n",
       " ['3', '-1.409078419', '-1.371631094', '3'],\n",
       " ['4', '-1.434896397', '-1.959709476', '4'],\n",
       " ['5', '3.706690333', '-0.803240913', '5']]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitRowsRDD = rowsRDD.map(lambda line: line.split(\",\"))\n",
    "splitRowsRDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`splitRowsRDD` elementų tipas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[list, list, list, list, list]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitRowsRDD.map(type).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`splitRowsRDD` elementų tipas yra _Python_ `list` (sąrašas). Kokie yra šių sąrašų tipai?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[str, str, str, str],\n",
       " [str, str, str, str],\n",
       " [str, str, str, str],\n",
       " [str, str, str, str],\n",
       " [str, str, str, str]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitRowsRDD.map(lambda list_: list(map(type, list_))).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matome, kad `splitRowsRDD` sudaro sąrašai su `str` tipo elementais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prieš atliekant skaičiavimus `str` tipo elementus reikia pavesti skaitinių tipų kintamaisiais:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, -0.485694686, 2.075174209, 1],\n",
       " [2, -0.3682471, 2.037708135, 2],\n",
       " [3, -1.409078419, -1.371631094, 3],\n",
       " [4, -1.434896397, -1.959709476, 4],\n",
       " [5, 3.706690333, -0.803240913, 5]]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "typedRowsRDD = splitRowsRDD.map(lambda vals: [int(vals[0]), float(vals[1]), float(vals[2]), int(vals[3])])\n",
    "typedRowsRDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ar pavyko?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[int, float, float, int],\n",
       " [int, float, float, int],\n",
       " [int, float, float, int],\n",
       " [int, float, float, int],\n",
       " [int, float, float, int]]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "typedRowsRDD.map(lambda list_: [type(element) for element in list_]).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Panašu, kad payko."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Apache Spark_ turi ir aukštesnio lygio abstrakciją - [`DataFrame`](http://spark.apache.org/docs/latest/sql-programming-guide.html#dataframes). Prieš turimą `RDD` paverčiant į `DataFrame` tipo objektą, `RDD` sąrašus pakeisime _pySpark_ `Row` tipo objektais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eilutę su stulpelių pavadinimais atskyrėme anksčiau. Eilutė:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ID,X,Y,klasteris'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stulpelių pavadnimų sąrašas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ID', 'X', 'Y', 'klasteris']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headerColumns = header.split(\",\")\n",
    "headerColumns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pagal turimus stulpelius sukuriame savo duomenims pritaikytą `Row`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Row(ID, X, Y, klasteris)>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca1Row = pyspark.sql.Row(\"ID\", \"X\", \"Y\", \"klasteris\")\n",
    "ca1Row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "arba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Row(ID, X, Y, klasteris)>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca1Row = pyspark.sql.Row(headerColumns[0], headerColumns[1], headerColumns[2], headerColumns[3])\n",
    "ca1Row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "arba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Row(ID, X, Y, klasteris)>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca1Row = pyspark.sql.Row(*headerColumns)\n",
    "ca1Row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sudarome `RDD` su `Row` tipo objektais:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ID=1, X=-0.485694686, Y=2.075174209, klasteris=1),\n",
       " Row(ID=2, X=-0.3682471, Y=2.037708135, klasteris=2),\n",
       " Row(ID=3, X=-1.409078419, Y=-1.371631094, klasteris=3),\n",
       " Row(ID=4, X=-1.434896397, Y=-1.959709476, klasteris=4),\n",
       " Row(ID=5, X=3.706690333, Y=-0.803240913, klasteris=5)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca1RowsRDD = typedRowsRDD.map(lambda values: ca1Row(values[0], values[1], values[2], values[3]))\n",
    "ca1RowsRDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "arba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ID=1, X=-0.485694686, Y=2.075174209, klasteris=1),\n",
       " Row(ID=2, X=-0.3682471, Y=2.037708135, klasteris=2),\n",
       " Row(ID=3, X=-1.409078419, Y=-1.371631094, klasteris=3),\n",
       " Row(ID=4, X=-1.434896397, Y=-1.959709476, klasteris=4),\n",
       " Row(ID=5, X=3.706690333, Y=-0.803240913, klasteris=5)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca1RowsRDD = typedRowsRDD.map(lambda values: ca1Row(*values))\n",
    "ca1RowsRDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ar tikrai pavyko?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[pyspark.sql.types.Row,\n",
       " pyspark.sql.types.Row,\n",
       " pyspark.sql.types.Row,\n",
       " pyspark.sql.types.Row,\n",
       " pyspark.sql.types.Row]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca1RowsRDD.map(type).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dabar galima `RDD` paversti į `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ID: bigint, X: double, Y: double, klasteris: bigint]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca1DF = ca1RowsRDD.toDF()\n",
    "ca1DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+------------+---------+\n",
      "| ID|           X|           Y|klasteris|\n",
      "+---+------------+------------+---------+\n",
      "|  1|-0.485694686| 2.075174209|        1|\n",
      "|  2|  -0.3682471| 2.037708135|        2|\n",
      "|  3|-1.409078419|-1.371631094|        3|\n",
      "|  4|-1.434896397|-1.959709476|        4|\n",
      "|  5| 3.706690333|-0.803240913|        5|\n",
      "+---+------------+------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ca1DF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puiku! `DataFrame` objektai bus naudojami su _Apache Spark_ _Machine Learning_ metodais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reikėjo atlikti nemažai darbo, kol iš tekstinio failo gavome `DataFrame` objektą. Galbūt šį procesą galime automatizuoti? Atliktus veiksmus sudedame į _Python_ funkciją:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from itertools import islice\n",
    "\n",
    "def parseCsvToDf(csvFilePath):\n",
    "    rawRdd = sc.textFile(csvFilePath)\n",
    "    header = rawRdd.first().split(\",\")\n",
    "    DataRow = Row(*[h.lower() for h in header])\n",
    "    rowsRdd = (\n",
    "        rawRdd\n",
    "        .mapPartitionsWithIndex(lambda idx, gen: islice(gen, 1, None) if idx == 0 else gen)\n",
    "        .map(lambda line: DataRow(*map(float, line.split(\",\"))))\n",
    "    )\n",
    "    rowDf = rowsRdd.toDF()\n",
    "    return rowDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasinaudojame ką tik aprašyta funkcija:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: double, x: double, y: double, klasteris: double]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca1ParsedDF = parseCsvToDf(\"data/CA1.csv\")\n",
    "ca1ParsedDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aprašydami funkciją padarėme prielaidą, kad visos `.csv` failo duomenų eilučių reikšmės yra skaitinės. Ar veiks ši funkcija, jeigu bent vienas stulpelis turės tekstines reikšmes? Pamėgikime. Įrašome testinį failą ir jį nuskaitome:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_filename = \"data/csv_file_with_strings.csv\"\n",
    "with open(test_filename, \"w\") as f:\n",
    "    f.writelines([\"stulpelis_a,stulpelis_b\\n\", \"simbolinė_reikšmė_1,10\\n\", \"simbolinė_reikšmė_2,20\\n\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patikrininame įrašyto failo turinį:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stulpelis_a,stulpelis_b\r\n",
      "simbolinė_reikšmė_1,10\r\n",
      "simbolinė_reikšmė_2,20\r\n"
     ]
    }
   ],
   "source": [
    "! cat data/csv_file_with_strings.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bandome įkelti failą naudodami savo aprašytą funkciją (įvyks klaida):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 18.0 failed 1 times, most recent failure: Lost task 0.0 in stage 18.0 (TID 18, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/srv/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/srv/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/srv/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/srv/spark/python/pyspark/rdd.py\", line 1295, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-48-2d1c5b82f951>\", line 11, in <lambda>\nValueError: could not convert string to float: 'simbolinė_reikšmė_1'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1822)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1835)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1848)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:393)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor79.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/srv/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/srv/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/srv/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/srv/spark/python/pyspark/rdd.py\", line 1295, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-48-2d1c5b82f951>\", line 11, in <lambda>\nValueError: could not convert string to float: 'simbolinė_reikšmė_1'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-259823d6bdd8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtestDF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparseCsvToDf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-48-2d1c5b82f951>\u001b[0m in \u001b[0;36mparseCsvToDf\u001b[1;34m(csvFilePath)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDataRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     )\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mrowDf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrowsRdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mrowDf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/srv/spark/python/pyspark/sql/context.py\u001b[0m in \u001b[0;36mtoDF\u001b[1;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'Alice'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \"\"\"\n\u001b[1;32m---> 62\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msampleRatio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[0mRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoDF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoDF\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/srv/spark/python/pyspark/sql/context.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[1;34m(self, data, schema, samplingRatio)\u001b[0m\n\u001b[0;32m    402\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 404\u001b[1;33m             \u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    405\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/srv/spark/python/pyspark/sql/context.py\u001b[0m in \u001b[0;36m_createFromRDD\u001b[1;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[0;32m    283\u001b[0m         \"\"\"\n\u001b[0;32m    284\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 285\u001b[1;33m             \u001b[0mstruct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inferSchema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    286\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m             \u001b[0mrdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/srv/spark/python/pyspark/sql/context.py\u001b[0m in \u001b[0;36m_inferSchema\u001b[1;34m(self, rdd, samplingRatio)\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mStructType\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m         \"\"\"\n\u001b[1;32m--> 229\u001b[1;33m         \u001b[0mfirst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfirst\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m             raise ValueError(\"The first row in RDD is empty, \"\n",
      "\u001b[1;32m/srv/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mfirst\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1315\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m         \"\"\"\n\u001b[1;32m-> 1317\u001b[1;33m         \u001b[0mrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1318\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/srv/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1298\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1299\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1301\u001b[0m             \u001b[0mitems\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/srv/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[0;32m    914\u001b[0m         \u001b[1;31m# SparkContext#runJob.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    915\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 916\u001b[1;33m         \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    917\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/srv/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[1;32m--> 538\u001b[1;33m                 self.target_id, self.name)\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/srv/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/srv/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    299\u001b[0m                     \u001b[1;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[0;32m    301\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 18.0 failed 1 times, most recent failure: Lost task 0.0 in stage 18.0 (TID 18, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/srv/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/srv/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/srv/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/srv/spark/python/pyspark/rdd.py\", line 1295, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-48-2d1c5b82f951>\", line 11, in <lambda>\nValueError: could not convert string to float: 'simbolinė_reikšmė_1'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1822)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1835)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1848)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:393)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor79.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/srv/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/srv/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/srv/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/srv/spark/python/pyspark/rdd.py\", line 1295, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-48-2d1c5b82f951>\", line 11, in <lambda>\nValueError: could not convert string to float: 'simbolinė_reikšmė_1'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "testDF = parseCsvToDf(test_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Klaidos pranešime matome eilutę:\n",
    "\n",
    "`ValueError: could not convert string to float: 'simbolinė_reikšmė_1'`\n",
    "\n",
    "Ši klaida reikškia, kad simbolių eilutės tipo (angl. string) reikšmės  `simbolinė_reikšmė_1` nepavyko paveiksti į skaitinio tipo `float` reikšmę."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Norėdami aprašyti funkciją, kuri patikrintų galimus variantus ir stulpeliams parinktų teisingus tipus, turėtume daryti panašiai kaip daroma [čia](https://github.com/seahboonsiew/pyspark-csv/blob/master/pyspark_csv.py). Tada turėtume universalią funkciją tekstinio `RDD` pavertinmui į `DataFrame`. Tačiau atminkime, kad _Python_ yra interpretuojama programavimo kalba, todėl gerokai lėtesnė už kompiliuojamą [_Scala_](http://www.scala-lang.org/) programavimo kalbą, kuria ir parašytas _Apache Spark_. Žemiau pateiktas šių kalbų spartos palyginimas darbui su _Apache Spark_ paimtas iš [šio šaltinio](https://databricks.com/blog/2015/04/24/recent-performance-improvements-in-apache-spark-sql-python-dataframes-and-more.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://databricks.com/wp-content/uploads/2015/02/Screen-Shot-2015-02-16-at-9.46.39-AM.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matome, kad naudojant `DataFrame` objektų metodus skirtumo tarp kalbų nėra, tačiau jis labai ryškus `RDD` objektų metodams. Žemiau pateikiame _Scala_ kalba parašytą _Apache Spark_ paketą, kurį galima naudoti iš _Python_ kalbos _Apache Spark_ sąsajos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [spark-csv](https://github.com/databricks/spark-csv) paketas iš http://spark-packages.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `.csv` failo įkėlimas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ID: int, X: double, Y: double, klasteris: int]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca1EasyDF = sqlContext.read.format(\"com.databricks.spark.csv\").options(header=True, inferSchema=True).load(\"data/CA1.csv\")\n",
    "ca1EasyDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+------------+---------+\n",
      "| ID|           X|           Y|klasteris|\n",
      "+---+------------+------------+---------+\n",
      "|  1|-0.485694686| 2.075174209|        1|\n",
      "|  2|  -0.3682471| 2.037708135|        2|\n",
      "|  3|-1.409078419|-1.371631094|        3|\n",
      "|  4|-1.434896397|-1.959709476|        4|\n",
      "|  5| 3.706690333|-0.803240913|        5|\n",
      "+---+------------+------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ca1EasyDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "arba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ID: int, X: double, Y: double, klasteris: int]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca1EasyDF = sqlContext.read.load(\"data/CA1.csv\", format=\"com.databricks.spark.csv\", header=True, inferSchema=True)\n",
    "ca1EasyDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+------------+---------+\n",
      "| ID|           X|           Y|klasteris|\n",
      "+---+------------+------------+---------+\n",
      "|  1|-0.485694686| 2.075174209|        1|\n",
      "|  2|  -0.3682471| 2.037708135|        2|\n",
      "|  3|-1.409078419|-1.371631094|        3|\n",
      "|  4|-1.434896397|-1.959709476|        4|\n",
      "|  5| 3.706690333|-0.803240913|        5|\n",
      "+---+------------+------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ca1EasyDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gavome ir stulpelių pavadinimus, ir jų tipus. Patikriname su testiniu failu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[stulpelis_a: string, stulpelis_b: int]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testEasyDF = sqlContext.read.load(\n",
    "    \"data/csv_file_with_strings.csv\", \n",
    "    format=\"com.databricks.spark.csv\", \n",
    "    header=True, inferSchema=True)\n",
    "testEasyDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+\n",
      "|        stulpelis_a|stulpelis_b|\n",
      "+-------------------+-----------+\n",
      "|simbolinė_reikšmė_1|         10|\n",
      "|simbolinė_reikšmė_2|         20|\n",
      "+-------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testEasyDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puiku!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tikrai rekomenduotina `.csv` failų įkėlimui naudoti pastarąjį metodą :) Parinkčių paaiškinimai pateikti [čia](https://github.com/databricks/spark-csv#features)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `DataFrame` išsaugojimas `.csv` formatu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testEasyDF.write.format(\"com.databricks.spark.csv\").save(\"data/csv_test_write\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Pastaba:_** _Apache Spark_ duomenis saugoja į direktoriją atskirais failais. Jeigu duomenų išsaugojimas atliktas sėktingai, direktorijoje sukuriamas tuščias failas `_SUCCESS`. Saugojimas negalimas, jeigu egzistuoja direktorija tokiu pačiu pavadinimu ir įvyks klaida \n",
    "\n",
    "`java.lang.RuntimeException: path data/csv_test_write already exists.`\n",
    "\n",
    "Išsaugoti galima tik duomenų eilutes (be stulpelių pavadinimų eilutės)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o233.save.\n: java.lang.RuntimeException: path data/csv_test_write already exists.\n\tat scala.sys.package$.error(package.scala:27)\n\tat com.databricks.spark.csv.DefaultSource.createRelation(DefaultSource.scala:157)\n\tat org.apache.spark.sql.execution.datasources.ResolvedDataSource$.apply(ResolvedDataSource.scala:170)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:146)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:137)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-a6e50a8cdf66>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# java.lang.RuntimeException: path data/csv_test_write already exists.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtestEasyDF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"com.databricks.spark.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/csv_test_write\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/srv/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m    330\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/srv/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[1;32m--> 538\u001b[1;33m                 self.target_id, self.name)\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/srv/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/srv/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    299\u001b[0m                     \u001b[1;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[0;32m    301\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o233.save.\n: java.lang.RuntimeException: path data/csv_test_write already exists.\n\tat scala.sys.package$.error(package.scala:27)\n\tat com.databricks.spark.csv.DefaultSource.createRelation(DefaultSource.scala:157)\n\tat org.apache.spark.sql.execution.datasources.ResolvedDataSource$.apply(ResolvedDataSource.scala:170)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:146)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:137)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "# java.lang.RuntimeException: path data/csv_test_write already exists.\n",
    "testEasyDF.write.format(\"com.databricks.spark.csv\").save(\"data/csv_test_write\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Žemiau pateikiama keletas komandų rezultatų peržiūrai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00000  _SUCCESS\r\n"
     ]
    }
   ],
   "source": [
    "# išvesti direktorijos turinį\n",
    "! ls data/csv_test_write/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simbolinė_reikšmė_1,10\r\n",
      "simbolinė_reikšmė_2,20\r\n"
     ]
    }
   ],
   "source": [
    "#išvesti pirmasias 2 failo eilutes\n",
    "! head -n 2 data/csv_test_write/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simbolinė_reikšmė_2,20\r\n"
     ]
    }
   ],
   "source": [
    "#išvesti paskutinę failo eilutę\n",
    "! tail -n 1 data/csv_test_write/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# failas _SUCCESS tikrai tuščias\n",
    "! cat data/csv_test_write/_SUCCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simbolinė_reikšmė_1,10\r\n",
      "simbolinė_reikšmė_2,20\r\n"
     ]
    }
   ],
   "source": [
    "# išvesti sujuntą visų direktorijos failų turinį\n",
    "! cat data/csv_test_write/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> data/csv_test_write/_SUCCESS <==\r\n",
      "\r\n",
      "==> data/csv_test_write/part-00000 <==\r\n",
      "simbolinė_reikšmė_1,10\r\n"
     ]
    }
   ],
   "source": [
    "# išvesti kiekvieno direktorijos failo pirmąją eilutę\n",
    "! head -n 1 data/csv_test_write/*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark 1.5.1 (Python 3.4)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
